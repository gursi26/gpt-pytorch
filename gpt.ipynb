{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import OpenAIGPTTokenizer\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, num_heads, p):\n",
    "        super(Attention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.c_attn = nn.Linear(input_dim, output_dim * 3)\n",
    "        self.c_proj = nn.Linear(output_dim, output_dim)\n",
    "        self.attn_dropout = nn.Dropout(p=p, inplace=False)\n",
    "        self.resid_dropout = nn.Dropout(p=p, inplace=False)\n",
    "\n",
    "    def scaled_dot_product_attention(self, q, k, v, mask: bool):\n",
    "        \"\"\"\n",
    "        q: [batch_size, num_heads, head_dim, seq1_len]\n",
    "        k: [batch_size, num_heads, head_dim, seq2_len]\n",
    "        v: [batch_size, num_heads, head_dim, seq2_len]\n",
    "        mask: [batch_size, num_heads, seq1_len, seq1_len]\n",
    "        (seq1_len = seq2_len for self attention)\n",
    "        \"\"\"\n",
    "        qk = q.matmul(k.transpose(-1, -2)) / math.sqrt(q.shape[-1])\n",
    "        if mask:\n",
    "            mask = torch.tril(torch.ones(1, 1, qk.shape[-2], qk.shape[-1])).type(torch.bool).to(qk.device)\n",
    "            qk = qk.masked_fill(~mask, -torch.inf)\n",
    "        attn_weights = self.attn_dropout(qk.softmax(dim=-1))\n",
    "        return attn_weights.matmul(v)\n",
    "    \n",
    "    def qkv_reshape(self, x):\n",
    "        return x.view(x.shape[0], x.shape[1], self.num_heads, -1).permute(0, 2, 1, 3)\n",
    "    \n",
    "    def output_reshape(self, x):\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "        return x.reshape(x.shape[0], x.shape[1], -1)\n",
    "    \n",
    "    def forward(self, x, mask: bool):\n",
    "        q, k, v = self.c_attn(x).chunk(3, dim=-1)\n",
    "        q, k, v = self.qkv_reshape(q), self.qkv_reshape(k), self.qkv_reshape(v)\n",
    "        attn_outputs = self.output_reshape(self.scaled_dot_product_attention(q, k, v, mask))\n",
    "        return self.resid_dropout(self.c_proj(attn_outputs))\n",
    "    \n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, p) -> None:\n",
    "        super(MLP, self).__init__()\n",
    "        self.c_fc = nn.Linear(input_dim, input_dim * 4)\n",
    "        self.c_proj = nn.Linear(input_dim * 4, input_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.dropout = nn.Dropout(p=p, inplace=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.act(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, p):\n",
    "        super(Block, self).__init__()\n",
    "        self.attn = Attention(d_model, d_model, num_heads, p)\n",
    "        self.ln_1 = nn.LayerNorm(d_model)\n",
    "        self.mlp = MLP(d_model, p)\n",
    "        self.ln_2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask: bool):\n",
    "        skip_x = x\n",
    "        x = self.attn(x, mask=mask)\n",
    "        x = self.ln_1(x + skip_x)\n",
    "        skip_x = x\n",
    "        x = self.mlp(x)\n",
    "        x = self.ln_2(x + skip_x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, max_seq_len, n_layers, d_model, num_heads, p):\n",
    "        super(GPT, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.tokens_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.positions_embed = nn.Embedding(max_seq_len, d_model)\n",
    "        self.drop = nn.Dropout(p=p, inplace=False)\n",
    "        self.h = nn.ModuleList([Block(d_model, num_heads, p) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        x = self.tokens_embed(x) * math.sqrt(self.d_model)\n",
    "        position_tokens = torch.arange(x.shape[-2]).unsqueeze(0).repeat(x.shape[0], 1).to(x.device)\n",
    "        x = self.drop(x + self.positions_embed(position_tokens))\n",
    "        for layer in self.h:\n",
    "            x = layer(x, mask=True)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model_and_tokenizer(weights_path, device):\n",
    "    tokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")\n",
    "    gpt = GPT(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        max_seq_len=512,\n",
    "        d_model=768,\n",
    "        num_heads=12,\n",
    "        n_layers=12,\n",
    "        p=0.1\n",
    "    ).to(device)\n",
    "    gpt.load_state_dict(torch.load(weights_path, map_location=device))\n",
    "    \n",
    "    special_tokens_dict = {\"bos_token\":\"<bos>\", \"eos_token\":\"<eos>\", \"sep_token\":\"<sep>\"}\n",
    "    num_added = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    new_embedding_weights = torch.randn(num_added, 768).to(device)\n",
    "    gpt.tokens_embed.weight.data = torch.cat([gpt.tokens_embed.weight.data, new_embedding_weights], dim=0)\n",
    "    return tokenizer, gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV = torch.device(\"mps\")\n",
    "tokenizer, model = init_model_and_tokenizer(\"weights.pth\", DEV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = tokenizer(\"<bos> hello, how are you? <sep> I am alright <eos>\", return_tensors=\"pt\")[\"input_ids\"].to(DEV)\n",
    "out = model(model_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
